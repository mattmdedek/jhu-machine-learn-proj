---
title: "Practical Machine Learning Course Project"
author: "Matt Dedek"
date: "Sunday, January 25, 2015"
output: html_document
---



```{r}
library(ggplot2)
library(caret)
library(plyr)
```
```
## Feature Extraction

I am using all raw features available to build the model. Derived metrics like totals and variances are excluded.

```{r}
predictors <- c(
  "total_accel_dumbbell",
  "total_accel_forearm",
  "gyros_forearm_x",
  "gyros_forearm_y",
  "gyros_forearm_z",
  "accel_forearm_x",
  "accel_forearm_y",
  "accel_forearm_z",
  "magnet_forearm_x",
  "magnet_forearm_y",
  "magnet_forearm_z",
  "gyros_dumbbell_x",
  "gyros_dumbbell_y",
  "gyros_dumbbell_z",
  "accel_dumbbell_x",
  "accel_dumbbell_y",
  "accel_dumbbell_z",
  "magnet_dumbbell_x",
  "magnet_dumbbell_y",
  "magnet_dumbbell_z",
  "roll_forearm",
  "pitch_forearm",
  "yaw_forearm",
  "roll_dumbbell",
  "pitch_dumbbell",
  "yaw_dumbbell",
  "gyros_arm_x",
  "gyros_arm_y",
  "gyros_arm_z",
  "accel_arm_x",
  "accel_arm_y",
  "accel_arm_z",
  "magnet_arm_x",
  "magnet_arm_y",
  "magnet_arm_z",
  "gyros_belt_x",
  "gyros_belt_y",
  "gyros_belt_z",
  "accel_belt_x",
  "accel_belt_y",
  "accel_belt_z",
  "magnet_belt_x",
  "magnet_belt_y",
  "magnet_belt_z",
  "roll_arm",
  "pitch_arm",
  "yaw_arm",
  "total_accel_arm",
  "roll_belt",
  "pitch_belt",
  "yaw_belt",
  "total_accel_belt"
)

training <- read.table("data/pml-training.csv", sep=",", header=T, quote="\"", na.strings=c("NA", "#DIV/0!", "\"\""))

trainDf <- training[,c(predictors, "classe")]
inTrain <- createDataPartition(y=trainDf$classe, p=0.7, list=F)
trainSet <- trainDf[inTrain,]
probeSet <- trainDf[-inTrain,]
```

## Model Training

I use the 'gbm' method of the caret package to apply a Random Forest with Bagging and cross-validation. Aggregating over multiple random forests reduces the variability of the models, multiple trees in the random forest gives the model more sensitivity and employing cross validation reduces overfitting.

Since the model takes a long time to build, it has been saved for future use, but the code used to generate the model is shown here:

```{r}
modFit <- train(classe ~ ., method="gbm", data=trainSet)
save(modFit, file="PredictionModel.Rda")
```

Load the trained model here:

```{r}
# Loads the variable modFit into the workspace
load("PredictionModel.Rda")
```

## Model Accuracy

Here I use the probe dataset to test accuracy.

```{r}
probePred <- predict(modFit, probeSet)
predCorrect <- (probeSet$classe == probePred)
nCorrect <- sum(predCorrect)
nIncorrect <- nrow(probeSet) - nCorrect
```

The expected accuracy is therefore: `r round(nCorrect * 100 / nrow(probeSet), 2)`%

## Generate files for Submission

```{r}
testing <- read.table("data/pml-testing.csv", sep=",", header=T, quote="\"", na.strings=c("NA", "#DIV/0!", "\"\""))
testPred <- predict(modFit, testing)

# Function provided by Prof. Leek
pml_write_files <- function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("submission/problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(testPred)
```

