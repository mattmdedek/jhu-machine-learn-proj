---
title: "Practical Machine Learning Course Project"
date: "Sunday, January 25, 2015"
output: html_document
---



```{r message=FALSE}
library(ggplot2)
library(caret)
library(plyr)
```

## Feature Extraction

I am using all of the raw features available to build the model. Derived metrics like total_*metric* and var_*metric* are excluded.

```{r}
predictors <- c(
  "total_accel_dumbbell",
  "total_accel_forearm",
  "gyros_forearm_x",
  "gyros_forearm_y",
  "gyros_forearm_z",
  "accel_forearm_x",
  "accel_forearm_y",
  "accel_forearm_z",
  "magnet_forearm_x",
  "magnet_forearm_y",
  "magnet_forearm_z",
  "gyros_dumbbell_x",
  "gyros_dumbbell_y",
  "gyros_dumbbell_z",
  "accel_dumbbell_x",
  "accel_dumbbell_y",
  "accel_dumbbell_z",
  "magnet_dumbbell_x",
  "magnet_dumbbell_y",
  "magnet_dumbbell_z",
  "roll_forearm",
  "pitch_forearm",
  "yaw_forearm",
  "roll_dumbbell",
  "pitch_dumbbell",
  "yaw_dumbbell",
  "gyros_arm_x",
  "gyros_arm_y",
  "gyros_arm_z",
  "accel_arm_x",
  "accel_arm_y",
  "accel_arm_z",
  "magnet_arm_x",
  "magnet_arm_y",
  "magnet_arm_z",
  "gyros_belt_x",
  "gyros_belt_y",
  "gyros_belt_z",
  "accel_belt_x",
  "accel_belt_y",
  "accel_belt_z",
  "magnet_belt_x",
  "magnet_belt_y",
  "magnet_belt_z",
  "roll_arm",
  "pitch_arm",
  "yaw_arm",
  "total_accel_arm",
  "roll_belt",
  "pitch_belt",
  "yaw_belt",
  "total_accel_belt"
)

# Read the input file
training <- read.table("data/pml-training.csv", sep=",", header=T, quote="\"", na.strings=c("NA", "#DIV/0!", "\"\""))

# Subset the data to only look at predictors of interest
trainDf <- training[,c(predictors, "classe")]

# Split the training data into a training set (70%) and a probe set (30%)
# The probe set will be used to estimate accuracy.
inTrain <- createDataPartition(y=trainDf$classe, p=0.7, list=F)
trainSet <- trainDf[inTrain,]
probeSet <- trainDf[-inTrain,]
```

## Model Training

I use the 'gbm' method of the caret package to implement Bagging with trees. Caret uses cross validation when building the model by default.

Bagging means that the model will aggregating multiple random forests, this reduces the variability of the final model. The random forest algorithm gives the model more sensitivity by randomizing the predictors. The use of cross-validation reduces overfitting and produces a more generalizable model.

Since this model takes a long time to build (about 30 minutes), I saved it for future use. The code used to generate the model is shown here.

```{r eval=FALSE}
modFit <- train(classe ~ ., method="gbm", data=trainSet)
save(modFit, file="PredictionModel.Rda")
```

This line loads the saved model.

```{r}
# Loads the variable modFit into the workspace
load("PredictionModel.Rda")
```

## Model Accuracy

As mentioned above, a 'probe' dataset was set aside to test the accuracy of the model. Here I use the predict function to predict the 'classe' of each exercise in the probe data set and compare the prediction to the expected value.

```{r message=FALSE, warning=FALSE}
probePred <- predict(modFit, probeSet)
predCorrect <- (probeSet$classe == probePred)
nCorrect <- sum(predCorrect)
nObs <- nrow(probeSet)
pcAccuracy <- round(nCorrect * 100 / nObs, 2)
```

The expected accuracy of this model is : `r pcAccuracy`%. Not too shabby considering it took one line of code and some patience.

Here is an additional table comparing the predictions to the expected values:

```{r}
table(probeSet$classe, probePred)
```

## Generate files for Submission

This code was used to create the files for the Submission part of the course project. 20/20 predictions were correct.


```{r}
testing <- read.table("data/pml-testing.csv", sep=",", header=T, quote="\"", na.strings=c("NA", "#DIV/0!", "\"\""))
testPred <- predict(modFit, testing)

# Function provided by Prof. Leek
pml_write_files <- function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("submission/problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(testPred)
```

